{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6ead3e",
   "metadata": {},
   "source": [
    "# Analyzing Review Text\n",
    "Extract the sentiment (positive or negative) and gain insight from Yelp review text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244b1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\n",
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = 'yelp.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "stars = [row['stars'] for row in data]\n",
    "print(data[0]['text'])\n",
    "print(stars[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b43b71",
   "metadata": {},
   "source": [
    "Build a linear model predicting the star rating based on the text reviews. \n",
    "Apply the bag-of-words model using the CountVectorizer to produce a feature matrix giving the counts of each word in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f5b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# predicting the star rating based on the text reviews\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# convert dict to df\n",
    "class ToDataFrame(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return a pandas data frame from X\n",
    "        X = pd.DataFrame.from_dict(X)\n",
    "        return X\n",
    "\n",
    "#check the function works and give us the right format\n",
    "to_data_frame = ToDataFrame()\n",
    "X_t = to_data_frame.fit_transform(data[:5])\n",
    "print((X_t == pd.DataFrame(data[:5])).all(axis=None))\n",
    "\n",
    "selector = ColumnTransformer(\n",
    "    transformers=[('text', to_data_frame, ['text'])\n",
    "])\n",
    "expected = np.array([data[0]['text']])\n",
    "\n",
    "# Check the selector returns just two columns, the latitude and longitude\n",
    "print((selector.fit_transform(X_t)[0] == expected).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b26a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poho/Documents/Software/anaconda3/envs/mlp/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#tune and get hyperparameter for countvectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = data\n",
    "y = [row['stars'] for row in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "to_data_frame = ToDataFrame()\n",
    "ridge = Ridge()\n",
    "text_vectorizer = CountVectorizer()\n",
    "\n",
    "param_grid = {\n",
    "    'ct1__text__text_vectorizer__max_df': [0.1, 0.25, 0.5, 0.6],\n",
    "    'ct1__text__text_vectorizer__min_df': [5,10,20,30]\n",
    "    \n",
    "    } \n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "    ('text_vectorizer', text_vectorizer)])\n",
    "\n",
    "ct1 = ColumnTransformer([('text', pipe1, 'text')])\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    [('to_data_frame', to_data_frame), \n",
    "     ('ct1', ct1), \n",
    "     ('ridge', ridge)])\n",
    "\n",
    "gs = GridSearchCV(pipe2, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, error_score='raise')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune and get hyperparameter for ridge regression\n",
    "\n",
    "# selector = ColumnTransformer(\n",
    "#     transformers=[('text_vectorizer', text_vectorizer, ['text'])\n",
    "# ])\n",
    "\n",
    "# pipe = Pipeline(\n",
    "#     [('to_data_frame', to_data_frame), \n",
    "#      ('select', selector), \n",
    "#      ('ridge', ridge)])\n",
    "\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "# pipe.predict(X_train[:20])\n",
    "\n",
    "# gs = GridSearchCV(pipe2, {'ridge__alpha': np.logspace(-1.0, 1.0, num=7)})\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_params_)\n",
    "\n",
    "#{'ridge__alpha': 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in final hyperparameter and train the model\n",
    "text_vectorizer = CountVectorizer(max_df=0.6, min_df=30)\n",
    "ridge = Ridge(alpha=10)\n",
    "\n",
    "X = data\n",
    "y = [row['stars'] for row in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, stars, test_size=0.33)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "    ('text_vectorizer', text_vectorizer)])\n",
    "\n",
    "ct1 = ColumnTransformer([('text', pipe1, 'text')])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    [('to_data_frame', to_data_frame), \n",
    "     ('ct1', ct1), \n",
    "     ('ridge', ridge)])\n",
    "\n",
    "bag_of_words_model = pipe2.fit(X_train, y_train)\n",
    "bag_of_words_model.predict(X_test)\n",
    "\n",
    "bag_of_words_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578088c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Predict on the test data\n",
    "predictions = bag_of_words_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}, R² score: {r2}\")\n",
    "# About 47.7% of the variance in the target variable can be explained by the features used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6459d",
   "metadata": {},
   "source": [
    "Bigram_model\n",
    "Consider word pairs and use a vectorizer(TfidfVectorizer) that applies normalization to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf inverse document frequency: the more this word happens among document, the less important it is, \n",
    "# coz just a generic word\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "to_data_frame = ToDataFrame()\n",
    "ridge = Ridge()\n",
    "text_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "tfidf_transformer = TfidfTransformer()  # Add TfidfTransformer\n",
    "\n",
    "X = data\n",
    "y = [row['stars'] for row in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, stars, test_size=0.33)\n",
    "\n",
    "\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "    ('text_vectorizer', text_vectorizer),\n",
    "    ('tfidf_transformer', tfidf_transformer)\n",
    "])\n",
    "\n",
    "ct1 = ColumnTransformer([('text', pipe1, 'text')])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    [('to_data_frame', to_data_frame), \n",
    "     ('ct1', ct1), \n",
    "     ('ridge', ridge)])\n",
    "\n",
    "\n",
    "bigram_model = pipe2.fit(X_train, y_train)\n",
    "bigram_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ece934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "predictions = bigram_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}, R² score: {r2}\")\n",
    "# About 63.9% of the variance in the target variable can be explained by the features used in the model, much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bfb51",
   "metadata": {},
   "source": [
    "Find the top 25 \"polarizing words\" in the corpus of reviews for both positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the \"most polar\" reviews\n",
    "polar_data = [row for row in data if row.get('stars') == 1 or row.get('stars') == 5]\n",
    "polar_star = [row['stars'] for row in polar_data]\n",
    "set(polar_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the naive Bayes model, MultinomialNB, TF-IDF weighting, remove stop words.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Filter the collection to only keep one-star and five-star reviews\n",
    "polar_data = [row for row in data if row.get('stars') == 1 or row.get('stars') == 5]\n",
    "\n",
    "# Convert the polar data to a DataFrame\n",
    "polar_df = pd.DataFrame(polar_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(polar_df['text'], polar_df['stars'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline with TF-IDF weighting, removing stop words, and Multinomial Naive Bayes\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=list(STOP_WORDS))),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature names from TF-IDF vectorizer\n",
    "feature_names = text_clf.named_steps['tfidf'].get_feature_names_out()\n",
    "\n",
    "# Get the log probabilities from the Multinomial Naive Bayes model\n",
    "log_probs = text_clf.named_steps['clf'].feature_log_prob_\n",
    "\n",
    "# Combine feature names with log probabilities\n",
    "word_log_prob_pairs = zip(feature_names, log_probs[1] - log_probs[0])  # Calculate the difference for positive class\n",
    "\n",
    "# Sort the pairs by log probability values (ascending for least positive, descending for most positive)\n",
    "sorted_word_log_probs = sorted(word_log_prob_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the top 25 most positive and least positive words\n",
    "most_positive_words = [word for word, _ in sorted_word_log_probs[:25]]\n",
    "least_positive_words = [word for word, _ in sorted_word_log_probs[-25:]]\n",
    "\n",
    "# Display the results\n",
    "print('Most Positive Words:', most_positive_words)\n",
    "print('Least Positive Words:', least_positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ed518",
   "metadata": {},
   "source": [
    "Look over all reviews of restaurants. We want to find collocations --- that is, bigrams that are \"special\" and appear more often than you'd expect from chance. Find word pairs that are unlikely to occur consecutively based on the underlying probability of their words: Basically find high p(w1w2)/p(w1)p(w2) for Top 100 bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a98a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = 'biz_data.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    business_data = json.load(json_file)\n",
    "\n",
    "len(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = [business for business in business_data if 'categories' in business and any('restaurants' in category.lower() for category in business['categories'])]\n",
    "restaurant_ids = [b['business_id'] for b in restaurants]\n",
    "restaurant_reviews = [row['text'] for row in data if row['business_id'] in restaurant_ids]\n",
    "len(restaurant_ids) #12876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308439a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_data = [row for row in data if row['business_id'] in restaurant_ids]\n",
    "restaurant_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae801d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res_df = pd.DataFrame(restaurant_data)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to compute the word and bigram frequencies for the documents in the corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "CV = CountVectorizer(min_df = 10)\n",
    "count_vector = CV.fit_transform(restaurant_reviews)\n",
    "print(count_vector.shape)\n",
    "\n",
    "CV2 = CountVectorizer(ngram_range = (2, 2), min_df = 10)\n",
    "count_vector2 = CV2.fit_transform(restaurant_reviews)\n",
    "print(count_vector2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd62ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total word frequencies and bigram frequencies across all documents in the corpus \n",
    "# Then convert the results into single-column dataframes and then dictionaries\n",
    "\n",
    "count_vector.toarray()\n",
    "count_vector.sum(axis = 0)\n",
    "pd.DataFrame(count_vector.sum(axis = 0), columns = CV.get_feature_names_out())\n",
    "word_freqs_df = pd.DataFrame(count_vector.sum(axis = 0), columns = CV.get_feature_names_out()).T\n",
    "word_freqs_df\n",
    "word_freqs_dict = word_freqs_df.to_dict()[0]\n",
    "bigram_freqs_df = pd.DataFrame(count_vector2.sum(axis = 0), columns = CV2.get_feature_names_out()).T\n",
    "bigram_freqs_df\n",
    "bigram_freqs_dict = bigram_freqs_df.to_dict()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total number of words, unique words, and bigrams across all documents; \n",
    "# Calculate the uniform word probability as 1/total unique words\n",
    "\n",
    "CV_all = CountVectorizer()\n",
    "count_vector_all = CV_all.fit_transform(restaurant_reviews)\n",
    "total_words = count_vector_all.sum()\n",
    "total_unique_words = count_vector_all.shape[1]\n",
    "\n",
    "CV2_all = CountVectorizer(ngram_range = (2, 2))\n",
    "count_vector2_all = CV2_all.fit_transform(restaurant_reviews)\n",
    "total_bigrams = count_vector2_all.sum()\n",
    "\n",
    "print(f\"There are totally {total_words} words and {total_bigrams} bigrams in the reviews.  The number of unique words is {total_unique_words}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d985b35",
   "metadata": {},
   "source": [
    "$\\dfrac{f}{17077355} \\cdot (1 - s) + \\dfrac{1}{87880} \\cdot s \\approx \\dfrac{f+30}{17077355}$.\n",
    "\n",
    "Let $f = 10$, we have:\n",
    "\n",
    "$\\dfrac{10}{17077355} \\cdot (1 - s) + \\dfrac{1}{87880} \\cdot s \\approx \\dfrac{40}{17077355}$,\n",
    "\n",
    "$\\dfrac{10}{17077355} \\cdot (-s) + \\dfrac{1}{87880} \\cdot s \\approx \\dfrac{30}{17077355}$,\n",
    "\n",
    "$\\left(\\dfrac{1}{87880} - \\dfrac{10}{17077355}\\right)s \\approx \\dfrac{30}{17077355}$,\n",
    "\n",
    "$s \\approx 0.16275525810789912$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = (30/17077355) / (1/87880 - 10/17077355)\n",
    "print(smoothing)\n",
    "\n",
    "uniform_word_prob = 1 / total_unique_words\n",
    "print(f\"Uniform word probability: {uniform_word_prob}\")\n",
    "\n",
    "# Check that each bigram is contain exactly one space\n",
    "import collections\n",
    "bigrams = list(bigram_freqs_dict.keys())\n",
    "space_counts = collections.Counter([sum(ch == ' ' for ch in bigram) for bigram in bigrams])\n",
    "space_counts\n",
    "space_counts[1] == len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the statistic for each bigram, with the specified smoothing_factor applied\n",
    "\n",
    "def get_top100(smoothing_factor: float) -> list:\n",
    "    prob_ratios = []\n",
    "    \n",
    "    for i in range(len(bigrams)):\n",
    "        w1w2 = bigrams[i]\n",
    "        w1 = bigrams_split[i][0]\n",
    "        w2 = bigrams_split[i][1]\n",
    "        \n",
    "        p_w1w2 = bigram_freqs_dict[w1w2] / total_bigrams\n",
    "        p_w1 = (word_freqs_dict[w1] / total_words) * (1. - smoothing_factor) + uniform_word_prob * smoothing_factor\n",
    "        p_w2 = (word_freqs_dict[w2] / total_words) * (1. - smoothing_factor) + uniform_word_prob * smoothing_factor\n",
    "        \n",
    "        prob_ratios.append(p_w1w2 / (p_w1 * p_w2))\n",
    "    \n",
    "    prob_ratios_df = pd.DataFrame(prob_ratios, index = bigrams)\n",
    "    prob_ratios_df = prob_ratios_df.sort_values(by = 0, ascending = False)\n",
    "    \n",
    "    top100 = list(prob_ratios_df.index[:100])\n",
    "    \n",
    "    return top100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_split = [bigram.split(' ') for bigram in bigrams]\n",
    "\n",
    "smoothing_factors = [0.16275525810789912, 0.75, 0.85, 0.95, 0.99]\n",
    "for sf in smoothing_factors:\n",
    "    print(f\"Smoothing factor: {sf}\")\n",
    "    print(f\"Top 100 bigrams: {get_top100(sf)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2362e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = get_top100(smoothing) # smoothing = 0.16275525810789912\n",
    "print(top100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
