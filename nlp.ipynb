{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6ead3e",
   "metadata": {},
   "source": [
    "# Analyzing Review Text\n",
    "Extract the sentiment (positive or negative) and gain insight from Yelp review text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244b1561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\n",
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = 'yelp.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "stars = [row['stars'] for row in data]\n",
    "print(data[0]['text'])\n",
    "print(stars[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b43b71",
   "metadata": {},
   "source": [
    "Build a linear model predicting the star rating based on the text reviews. \n",
    "Apply the bag-of-words model using the CountVectorizer to produce a feature matrix giving the counts of each word in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f5b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# predicting the star rating based on the text reviews\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# convert dict to df\n",
    "class ToDataFrame(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return a pandas data frame from X\n",
    "        X = pd.DataFrame.from_dict(X)\n",
    "        return X\n",
    "\n",
    "#check the function works and give us the right format\n",
    "to_data_frame = ToDataFrame()\n",
    "X_t = to_data_frame.fit_transform(data[:5])\n",
    "print((X_t == pd.DataFrame(data[:5])).all(axis=None))\n",
    "\n",
    "selector = ColumnTransformer(\n",
    "    transformers=[('text', to_data_frame, ['text'])\n",
    "])\n",
    "expected = np.array([data[0]['text']])\n",
    "\n",
    "# Check the selector returns just two columns, the latitude and longitude\n",
    "print((selector.fit_transform(X_t)[0] == expected).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b26a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poho/Documents/Software/anaconda3/envs/mlp/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9223282322731482\n",
      "{'ct1__text__text_vectorizer__max_df': 0.6, 'ct1__text__text_vectorizer__min_df': 30}\n"
     ]
    }
   ],
   "source": [
    "#tune and get hyperparameter for countvectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = data\n",
    "y = [row['stars'] for row in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "to_data_frame = ToDataFrame()\n",
    "ridge = Ridge()\n",
    "text_vectorizer = CountVectorizer()\n",
    "\n",
    "param_grid = {\n",
    "    'ct1__text__text_vectorizer__max_df': [0.1, 0.25, 0.5, 0.6],\n",
    "    'ct1__text__text_vectorizer__min_df': [5,10,20,30]\n",
    "    \n",
    "    } \n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "    ('text_vectorizer', text_vectorizer)])\n",
    "\n",
    "ct1 = ColumnTransformer([('text', pipe1, 'text')])\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    [('to_data_frame', to_data_frame), \n",
    "     ('ct1', ct1), \n",
    "     ('ridge', ridge)])\n",
    "\n",
    "gs = GridSearchCV(pipe2, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, error_score='raise')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18e8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune and get hyperparameter for ridge regression\n",
    "\n",
    "# selector = ColumnTransformer(\n",
    "#     transformers=[('text_vectorizer', text_vectorizer, ['text'])\n",
    "# ])\n",
    "\n",
    "# pipe = Pipeline(\n",
    "#     [('to_data_frame', to_data_frame), \n",
    "#      ('select', selector), \n",
    "#      ('ridge', ridge)])\n",
    "\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "# pipe.predict(X_train[:20])\n",
    "\n",
    "# gs = GridSearchCV(pipe2, {'ridge__alpha': np.logspace(-1.0, 1.0, num=7)})\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_params_)\n",
    "\n",
    "#{'ridge__alpha': 10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8707b144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;to_data_frame&#x27;, ToDataFrame()),\n",
       "                (&#x27;ct1&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;text_vectorizer&#x27;,\n",
       "                                                                   CountVectorizer(max_df=0.6,\n",
       "                                                                                   min_df=30))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;ridge&#x27;, Ridge(alpha=10))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;to_data_frame&#x27;, ToDataFrame()),\n",
       "                (&#x27;ct1&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;text_vectorizer&#x27;,\n",
       "                                                                   CountVectorizer(max_df=0.6,\n",
       "                                                                                   min_df=30))]),\n",
       "                                                  &#x27;text&#x27;)])),\n",
       "                (&#x27;ridge&#x27;, Ridge(alpha=10))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ToDataFrame</label><div class=\"sk-toggleable__content\"><pre>ToDataFrame()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ct1: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;text&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;text_vectorizer&#x27;,\n",
       "                                                  CountVectorizer(max_df=0.6,\n",
       "                                                                  min_df=30))]),\n",
       "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">text</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.6, min_df=30)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=10)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('to_data_frame', ToDataFrame()),\n",
       "                ('ct1',\n",
       "                 ColumnTransformer(transformers=[('text',\n",
       "                                                  Pipeline(steps=[('text_vectorizer',\n",
       "                                                                   CountVectorizer(max_df=0.6,\n",
       "                                                                                   min_df=30))]),\n",
       "                                                  'text')])),\n",
       "                ('ridge', Ridge(alpha=10))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put in final hyperparameter and train the model\n",
    "text_vectorizer = CountVectorizer(max_df=0.6, min_df=30)\n",
    "ridge = Ridge(alpha=10)\n",
    "\n",
    "X = data\n",
    "y = [row['stars'] for row in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, stars, test_size=0.33)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "    ('text_vectorizer', text_vectorizer)])\n",
    "\n",
    "ct1 = ColumnTransformer([('text', pipe1, 'text')])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    [('to_data_frame', to_data_frame), \n",
    "     ('ct1', ct1), \n",
    "     ('ridge', ridge)])\n",
    "\n",
    "bag_of_words_model = pipe2.fit(X_train, y_train)\n",
    "bag_of_words_model.predict(X_test)\n",
    "\n",
    "bag_of_words_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578088c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.7450471336743405, RMSE: 0.8631611284542072, R² score: 0.559313122706556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Predict on the test data\n",
    "predictions = bag_of_words_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}, R² score: {r2}\")\n",
    "# About 47.7% of the variance in the target variable can be explained by the features used in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6459d",
   "metadata": {},
   "source": [
    "Bigram_model\n",
    "Consider word pairs and use a vectorizer(TfidfVectorizer) that applies normalization to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2010d6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.24013401, 4.26779933, 3.96754028, ..., 2.11079104, 3.84066754,\n",
       "       2.62935204])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idf inverse document frequency: the more this word happens among document, the less important it is, \n",
    "# coz just a generic word\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "to_data_frame = ToDataFrame()\n",
    "ridge = Ridge()\n",
    "text_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "tfidf_transformer = TfidfTransformer()  # Add TfidfTransformer\n",
    "\n",
    "X = data\n",
    "y = [row['stars'] for row in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, stars, test_size=0.33)\n",
    "\n",
    "\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "    ('text_vectorizer', text_vectorizer),\n",
    "    ('tfidf_transformer', tfidf_transformer)\n",
    "])\n",
    "\n",
    "ct1 = ColumnTransformer([('text', pipe1, 'text')])\n",
    "\n",
    "\n",
    "pipe2 = Pipeline(\n",
    "    [('to_data_frame', to_data_frame), \n",
    "     ('ct1', ct1), \n",
    "     ('ridge', ridge)])\n",
    "\n",
    "\n",
    "bigram_model = pipe2.fit(X_train, y_train)\n",
    "bigram_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ece934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.6091988671587235, RMSE: 0.7805119263398372, R² score: 0.6399759926056632\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "predictions = bigram_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}, R² score: {r2}\")\n",
    "# About 63.9% of the variance in the target variable can be explained by the features used in the model, much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bfb51",
   "metadata": {},
   "source": [
    "Find the top 25 \"polarizing words\" in the corpus of reviews for both positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e7b457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the \"most polar\" reviews\n",
    "polar_data = [row for row in data if row.get('stars') == 1 or row.get('stars') == 5]\n",
    "polar_star = [row['stars'] for row in polar_data]\n",
    "set(polar_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac3094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Words: ['perfection', 'delicious', 'fantastic', 'gem', 'yummy', 'impeccable', 'amazing', 'excellent', 'yum', 'delish', 'perfect', 'outstanding', 'awesome', 'refreshing', 'notch', 'incredible', 'wonderful', 'perfectly', 'terrific', 'superb', 'favorite', 'loved', 'heaven', 'favorites', 'divine']\n",
      "Least Positive Words: ['disgusted', 'lukewarm', 'lousy', 'rancid', 'crooks', 'unfriendly', 'blamed', 'inedible', 'rudest', 'insult', 'disrespectful', 'terrible', 'refund', 'awful', 'disgusting', 'rude', 'tasteless', 'rudely', 'incompetent', 'poisoning', 'horrible', 'unhelpful', 'unprofessional', 'unacceptable', 'worst']\n"
     ]
    }
   ],
   "source": [
    "# Use the naive Bayes model, MultinomialNB, TF-IDF weighting, remove stop words.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Filter the collection to only keep one-star and five-star reviews\n",
    "polar_data = [row for row in data if row.get('stars') == 1 or row.get('stars') == 5]\n",
    "\n",
    "# Convert the polar data to a DataFrame\n",
    "polar_df = pd.DataFrame(polar_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(polar_df['text'], polar_df['stars'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline with TF-IDF weighting, removing stop words, and Multinomial Naive Bayes\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=list(STOP_WORDS))),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature names from TF-IDF vectorizer\n",
    "feature_names = text_clf.named_steps['tfidf'].get_feature_names_out()\n",
    "\n",
    "# Get the log probabilities from the Multinomial Naive Bayes model\n",
    "log_probs = text_clf.named_steps['clf'].feature_log_prob_\n",
    "\n",
    "# Combine feature names with log probabilities\n",
    "word_log_prob_pairs = zip(feature_names, log_probs[1] - log_probs[0])  # Calculate the difference for positive class\n",
    "\n",
    "# Sort the pairs by log probability values (ascending for least positive, descending for most positive)\n",
    "sorted_word_log_probs = sorted(word_log_prob_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the top 25 most positive and least positive words\n",
    "most_positive_words = [word for word, _ in sorted_word_log_probs[:25]]\n",
    "least_positive_words = [word for word, _ in sorted_word_log_probs[-25:]]\n",
    "\n",
    "# Display the results\n",
    "print('Most Positive Words:', most_positive_words)\n",
    "print('Least Positive Words:', least_positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ed518",
   "metadata": {},
   "source": [
    "Look over all reviews of restaurants. We want to find collocations --- that is, bigrams that are \"special\" and appear more often than you'd expect from chance. Find word pairs that are unlikely to occur consecutively based on the underlying probability of their words: Basically find high p(w1w2)/p(w1)p(w2) for Top 100 bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a98a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37938"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_path = 'biz_data.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    business_data = json.load(json_file)\n",
    "\n",
    "len(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9fc5d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12876"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants = [business for business in business_data if 'categories' in business and any('restaurants' in category.lower() for category in business['categories'])]\n",
    "restaurant_ids = [b['business_id'] for b in restaurants]\n",
    "restaurant_reviews = [row['text'] for row in data if row['business_id'] in restaurant_ids]\n",
    "len(restaurant_ids) #12876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308439a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'votes': {'funny': 6, 'useful': 0, 'cool': 0},\n",
       " 'user_id': 'ZYaumz29bl9qHpu-KVtMGA',\n",
       " 'review_id': 'ow1c4Lcl3ObWxDC2yurwjQ',\n",
       " 'stars': 4,\n",
       " 'date': '2009-05-04',\n",
       " 'text': \"If you like lot lizards, you'll love the Pine Cone!\",\n",
       " 'type': 'review',\n",
       " 'business_id': 'JwUE5GmEO-sH1FuwJgKBlQ'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_data = [row for row in data if row['business_id'] in restaurant_ids]\n",
    "restaurant_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae801d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       " 'full_address': '4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018',\n",
       " 'hours': {'Tuesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Friday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Monday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Wednesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Thursday': {'close': '17:00', 'open': '08:00'}},\n",
       " 'open': True,\n",
       " 'categories': ['Doctors', 'Health & Medical'],\n",
       " 'city': 'Phoenix',\n",
       " 'review_count': 7,\n",
       " 'name': 'Eric Goldberg, MD',\n",
       " 'neighborhoods': [],\n",
       " 'longitude': -111.983758,\n",
       " 'state': 'AZ',\n",
       " 'stars': 3.5,\n",
       " 'latitude': 33.499313,\n",
       " 'attributes': {'By Appointment Only': True},\n",
       " 'type': 'business'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a305754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>votes</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>business_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'funny': 6, 'useful': 0, 'cool': 0}</td>\n",
       "      <td>ZYaumz29bl9qHpu-KVtMGA</td>\n",
       "      <td>ow1c4Lcl3ObWxDC2yurwjQ</td>\n",
       "      <td>4</td>\n",
       "      <td>2009-05-04</td>\n",
       "      <td>If you like lot lizards, you'll love the Pine ...</td>\n",
       "      <td>review</td>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "      <td>EEYwj6_t1OT5WQGypqEPNg</td>\n",
       "      <td>4iPPOQIo5Mr1NAUPUgCUrQ</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>Only went here once about a year and a half ag...</td>\n",
       "      <td>review</td>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'funny': 0, 'useful': 1, 'cool': 0}</td>\n",
       "      <td>MnXcXwr0keJpkIiwuPsOKg</td>\n",
       "      <td>_utPYHIdXeq8CqQ4iYD1bw</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>Ate a Saturday morning breakfast at the Pine C...</td>\n",
       "      <td>review</td>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'funny': 0, 'useful': 1, 'cool': 0}</td>\n",
       "      <td>wC8r-m6KHifL6R2i8ok8yg</td>\n",
       "      <td>gksnzyc9jQ9hNXESjvTrQw</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-08-26</td>\n",
       "      <td>This is definitely not your usual truck stop. ...</td>\n",
       "      <td>review</td>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "      <td>RvweNJFVkR3ttkWsIBy7nQ</td>\n",
       "      <td>PCa_K6ijV3Tzbp6nouEiJQ</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-03-13</td>\n",
       "      <td>I like this location better than the one near ...</td>\n",
       "      <td>review</td>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  votes                 user_id  \\\n",
       "0  {'funny': 6, 'useful': 0, 'cool': 0}  ZYaumz29bl9qHpu-KVtMGA   \n",
       "1  {'funny': 0, 'useful': 0, 'cool': 0}  EEYwj6_t1OT5WQGypqEPNg   \n",
       "2  {'funny': 0, 'useful': 1, 'cool': 0}  MnXcXwr0keJpkIiwuPsOKg   \n",
       "3  {'funny': 0, 'useful': 1, 'cool': 0}  wC8r-m6KHifL6R2i8ok8yg   \n",
       "4  {'funny': 0, 'useful': 0, 'cool': 0}  RvweNJFVkR3ttkWsIBy7nQ   \n",
       "\n",
       "                review_id  stars        date  \\\n",
       "0  ow1c4Lcl3ObWxDC2yurwjQ      4  2009-05-04   \n",
       "1  4iPPOQIo5Mr1NAUPUgCUrQ      4  2011-03-31   \n",
       "2  _utPYHIdXeq8CqQ4iYD1bw      3  2012-01-08   \n",
       "3  gksnzyc9jQ9hNXESjvTrQw      3  2012-08-26   \n",
       "4  PCa_K6ijV3Tzbp6nouEiJQ      4  2014-03-13   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  If you like lot lizards, you'll love the Pine ...  review   \n",
       "1  Only went here once about a year and a half ag...  review   \n",
       "2  Ate a Saturday morning breakfast at the Pine C...  review   \n",
       "3  This is definitely not your usual truck stop. ...  review   \n",
       "4  I like this location better than the one near ...  review   \n",
       "\n",
       "              business_id  \n",
       "0  JwUE5GmEO-sH1FuwJgKBlQ  \n",
       "1  JwUE5GmEO-sH1FuwJgKBlQ  \n",
       "2  JwUE5GmEO-sH1FuwJgKBlQ  \n",
       "3  JwUE5GmEO-sH1FuwJgKBlQ  \n",
       "4  JwUE5GmEO-sH1FuwJgKBlQ  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res_df = pd.DataFrame(restaurant_data)\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ffefbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143361, 19632)\n",
      "(143361, 155628)\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizer to compute the word and bigram frequencies for the documents in the corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "CV = CountVectorizer(min_df = 10)\n",
    "count_vector = CV.fit_transform(restaurant_reviews)\n",
    "print(count_vector.shape)\n",
    "\n",
    "CV2 = CountVectorizer(ngram_range = (2, 2), min_df = 10)\n",
    "count_vector2 = CV2.fit_transform(restaurant_reviews)\n",
    "print(count_vector2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dd62ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total word frequencies and bigram frequencies across all documents in the corpus \n",
    "# Then convert the results into single-column dataframes and then dictionaries\n",
    "\n",
    "count_vector.toarray()\n",
    "count_vector.sum(axis = 0)\n",
    "pd.DataFrame(count_vector.sum(axis = 0), columns = CV.get_feature_names_out())\n",
    "word_freqs_df = pd.DataFrame(count_vector.sum(axis = 0), columns = CV.get_feature_names_out()).T\n",
    "word_freqs_df\n",
    "word_freqs_dict = word_freqs_df.to_dict()[0]\n",
    "bigram_freqs_df = pd.DataFrame(count_vector2.sum(axis = 0), columns = CV2.get_feature_names_out()).T\n",
    "bigram_freqs_df\n",
    "bigram_freqs_dict = bigram_freqs_df.to_dict()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "881c2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 17077355 words and 16934005 bigrams in the reviews.  The number of unique words is 87880.\n"
     ]
    }
   ],
   "source": [
    "# Compute the total number of words, unique words, and bigrams across all documents; \n",
    "# Calculate the uniform word probability as 1/total unique words\n",
    "\n",
    "CV_all = CountVectorizer()\n",
    "count_vector_all = CV_all.fit_transform(restaurant_reviews)\n",
    "total_words = count_vector_all.sum()\n",
    "total_unique_words = count_vector_all.shape[1]\n",
    "\n",
    "CV2_all = CountVectorizer(ngram_range = (2, 2))\n",
    "count_vector2_all = CV2_all.fit_transform(restaurant_reviews)\n",
    "total_bigrams = count_vector2_all.sum()\n",
    "\n",
    "print(f\"There are totally {total_words} words and {total_bigrams} bigrams in the reviews.  The number of unique words is {total_unique_words}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d985b35",
   "metadata": {},
   "source": [
    "$\\dfrac{f}{17077355} \\cdot (1 - s) + \\dfrac{1}{87880} \\cdot s \\approx \\dfrac{f+30}{17077355}$.\n",
    "\n",
    "Let $f = 10$, we have:\n",
    "\n",
    "$\\dfrac{10}{17077355} \\cdot (1 - s) + \\dfrac{1}{87880} \\cdot s \\approx \\dfrac{40}{17077355}$,\n",
    "\n",
    "$\\dfrac{10}{17077355} \\cdot (-s) + \\dfrac{1}{87880} \\cdot s \\approx \\dfrac{30}{17077355}$,\n",
    "\n",
    "$\\left(\\dfrac{1}{87880} - \\dfrac{10}{17077355}\\right)s \\approx \\dfrac{30}{17077355}$,\n",
    "\n",
    "$s \\approx 0.16275525810789912$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc9a68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16275525810789912\n",
      "Uniform word probability: 1.137915339098771e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothing = (30/17077355) / (1/87880 - 10/17077355)\n",
    "print(smoothing)\n",
    "\n",
    "uniform_word_prob = 1 / total_unique_words\n",
    "print(f\"Uniform word probability: {uniform_word_prob}\")\n",
    "\n",
    "# Check that each bigram is contain exactly one space\n",
    "import collections\n",
    "bigrams = list(bigram_freqs_dict.keys())\n",
    "space_counts = collections.Counter([sum(ch == ' ' for ch in bigram) for bigram in bigrams])\n",
    "space_counts\n",
    "space_counts[1] == len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db1aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the statistic for each bigram, with the specified smoothing_factor applied\n",
    "\n",
    "def get_top100(smoothing_factor: float) -> list:\n",
    "    prob_ratios = []\n",
    "    \n",
    "    for i in range(len(bigrams)):\n",
    "        w1w2 = bigrams[i]\n",
    "        w1 = bigrams_split[i][0]\n",
    "        w2 = bigrams_split[i][1]\n",
    "        \n",
    "        p_w1w2 = bigram_freqs_dict[w1w2] / total_bigrams\n",
    "        p_w1 = (word_freqs_dict[w1] / total_words) * (1. - smoothing_factor) + uniform_word_prob * smoothing_factor\n",
    "        p_w2 = (word_freqs_dict[w2] / total_words) * (1. - smoothing_factor) + uniform_word_prob * smoothing_factor\n",
    "        \n",
    "        prob_ratios.append(p_w1w2 / (p_w1 * p_w2))\n",
    "    \n",
    "    prob_ratios_df = pd.DataFrame(prob_ratios, index = bigrams)\n",
    "    prob_ratios_df = prob_ratios_df.sort_values(by = 0, ascending = False)\n",
    "    \n",
    "    top100 = list(prob_ratios_df.index[:100])\n",
    "    \n",
    "    return top100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "352b21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing factor: 0.16275525810789912\n",
      "Top 100 bigrams: ['rula bula', 'knick knacks', 'dac biet', 'ropa vieja', 'feng shui', 'cien agaves', 'gulab jamun', 'himal chuli', 'tammie coe', 'itty bitty', 'riff raff', 'khai hoan', 'roka akor', 'patatas bravas', 'nanay gloria', 'baskin robbins', 'puerto rican', 'reina pepiada', 'chicha morada', 'wal mart', 'dueling pianos', 'hoity toity', 'haricot vert', 'tutti santi', 'hodge podge', 'luc lac', 'lomo saltado', 'bradley ogden', 'nuoc mam', 'valle luna', 'hu tieu', 'alain ducasse', 'vice versa', 'porta alba', 'har gow', 'kao tod', 'pina colada', 'krispy kreme', 'artery clogging', 'pura vida', 'ore ida', 'chino bandido', 'sous vide', 'celine dion', 'holyrood 9a', 'lloyd wright', 'pin kaow', 'harry potter', 'molecular gastronomy', 'ping pang', 'casey moore', 'malai kofta', 'deja vu', 'cochinita pibil', 'aguas frescas', 'kilt lifter', 'lactose intolerant', 'hors oeuvres', 'moscow mule', 'ama ebi', 'yada yada', 'thit nuong', 'womp womp', 'yadda yadda', 'scantily clad', 'demi glace', 'duct tape', 'lindo michoacan', 'tres leches', 'kee mao', 'woody allen', 'arnold palmer', 'kool aid', 'cabo wabo', 'coca cola', 'osso bucco', 'dom demarco', 'bi bim', 'frou frou', 'salo salo', 'ritz carlton', 'bok choy', 'mt everest', 'stainless steel', 'rick moonen', 'khao soi', 'va bene', 'prik ong', 'nem nuong', 'turo turo', 'hush puppies', 'huli huli', 'jean philippe', 'tsk tsk', 'panna cotta', 'betty boop', 'van buren', 'bim bap', 'toby keith', 'hong kong']\n",
      "\n",
      "Smoothing factor: 0.75\n",
      "Top 100 bigrams: ['pei wei', 'kung pao', 'ami gabi', 'planet hollywood', 'hong kong', 'foie gras', 'http www', 'prix fixe', 'bloody mary', 'amuse bouche', 'cole slaw', 'panna cotta', 'wicked spoon', 'dim sum', 'creme brulee', 'tex mex', 'los angeles', 'wi fi', 'carne asada', 'mon ami', 'huevos rancheros', 'tikka masala', 'clam chowder', 'hush puppies', 'tater tots', 'panda express', 'gordon ramsay', 'monte carlo', 'banh mi', 'butternut squash', 'pf changs', 'loco moco', 'wolfgang puck', 'mandalay bay', 'sea bass', 'beaten path', 'saving grace', 'pinot noir', 'com biz_photos', 'bok choy', 'nom nom', 'kool aid', 'joel robuchon', 'timely manner', 'pet peeve', 'michael mina', 'san francisco', 'cotton candy', 'croque madame', 'cave creek', 'hash browns', 'chow mein', 'san diego', 'shabu shabu', 'osso bucco', 'al dente', 'ruth chris', 'tres leches', 'dimly lit', 'co worker', 'al pastor', 'filet mignon', 'mahi mahi', 'au jus', 'del frisco', 'bo hue', 'au gratin', 'santa fe', 'hubert keller', 'co workers', 'chick fil', 'pizzeria bianco', 'brussel sprouts', 'lo mein', 'east coast', 'dac biet', 'circus circus', 'eiffel tower', 'bananas foster', 'julian serrano', 'pine nuts', 'conveyor belt', 'pf chang', 'thomas keller', 'arnold palmer', 'mario batali', 'cheesecake factory', 'pleasantly surprised', 'coca cola', 'flip flops', 'video poker', 'pita jungle', 'spam musubi', 'pina colada', 'monte cristo', 'moscow mule', 'bone marrow', 'gallo blanco', 'grand lux', 'gift certificate']\n",
      "\n",
      "Smoothing factor: 0.85\n",
      "Top 100 bigrams: ['http www', 'foie gras', 'carne asada', 'dim sum', 'bloody mary', 'planet hollywood', 'creme brulee', 'kung pao', 'cole slaw', 'pei wei', 'sea bass', 'wicked spoon', 'ami gabi', 'com biz_photos', 'clam chowder', 'prix fixe', 'panda express', 'los angeles', 'filet mignon', 'hash browns', 'mon ami', 'hong kong', 'amuse bouche', 'mandalay bay', 'tikka masala', 'gordon ramsay', 'tater tots', 'san francisco', 'prime rib', 'banh mi', 'panna cotta', 'butternut squash', 'chow mein', 'tex mex', 'san diego', 'pleasantly surprised', 'nom nom', 'wi fi', 'reasonably priced', 'timely manner', 'huevos rancheros', 'al pastor', 'co workers', 'co worker', 'saving grace', 'onion rings', 'east coast', 'al dente', 'pf changs', 'monte carlo', 'hush puppies', 'mahi mahi', 'cotton candy', 'loco moco', 'wolfgang puck', 'pinot noir', 'beaten path', 'cheesecake factory', 'shabu shabu', 'olive oil', 'michael mina', 'lo mein', 'pita jungle', 'mashed potatoes', 'au jus', 'cave creek', 'joel robuchon', 'hidden gem', 'ruth chris', 'dimly lit', 'au gratin', 'bone marrow', 'pizzeria bianco', 'bok choy', 'kool aid', 'pet peeve', 'brussel sprouts', 'sub par', 'stir fry', 'croque madame', 'grand lux', 'del frisco', 'grocery store', 'drive thru', 'crab legs', 'fellow yelpers', 'bananas foster', 'medium rare', 'la carte', 'pine nuts', 'gift certificate', 'osso bucco', 'bo hue', 'olive garden', 'hubert keller', 'pot stickers', 'tres leches', 'circus circus', 'eiffel tower', 'credit card']\n",
      "\n",
      "Smoothing factor: 0.95\n",
      "Top 100 bigrams: ['carne asada', 'http www', 'foie gras', 'dim sum', 'prime rib', 'sea bass', 'filet mignon', 'bloody mary', 'creme brulee', 'com biz_photos', 'mashed potatoes', 'ice cream', 'reasonably priced', 'onion rings', 'pleasantly surprised', 'crab legs', 'hash browns', 'medium rare', 'cole slaw', 'planet hollywood', 'olive oil', 'happy hour', 'wicked spoon', 'french toast', 'highly recommend', 'pad thai', 'drive thru', 'iced tea', 'las vegas', 'clam chowder', 'www yelp', 'gluten free', 'kung pao', 'panda express', 'co workers', 'top notch', 'east coast', 'san francisco', 'sub par', 'hidden gem', 'mandalay bay', 'los angeles', 'pei wei', 'yelp com', 'mon ami', 'san diego', 'ami gabi', 'eggs benedict', 'strip mall', 'chow mein', 'per person', 'rib eye', 'cheesecake factory', 'new york', 'al pastor', 'credit card', 'years ago', 'grocery store', 'pita jungle', 'co worker', 'spring rolls', 'thin crust', 'prix fixe', 'gordon ramsay', 'peanut butter', 'tater tots', 'tikka masala', 'olive garden', 'la carte', 'bone marrow', 'weeks ago', 'highly recommended', 'pulled pork', 'al dente', 'shopping center', 'banh mi', 'pork belly', 'outdoor seating', 'butternut squash', 'looking forward', 'mahi mahi', '24 hours', 'amuse bouche', 'sweet potato', 'stir fry', 'ahi tuna', 'nom nom', 'hong kong', 'short ribs', 'timely manner', 'lo mein', 'yourself favor', 'red velvet', 'fellow yelpers', 'taco bell', 'sin city', 'grand lux', 'walking distance', 'above average', 'smoked salmon']\n",
      "\n",
      "Smoothing factor: 0.99\n",
      "Top 100 bigrams: ['happy hour', 'ice cream', 'las vegas', 'prime rib', 'highly recommend', 'carne asada', 'crab legs', 'mashed potatoes', 'dim sum', 'http www', 'foie gras', 'french toast', 'medium rare', 'pad thai', 'sweet potato', 'reasonably priced', 'onion rings', 'this place', 've been', 'better than', 'ended up', 'fried rice', 'top notch', 'filet mignon', 'next time', 'as well', 've ever', 'sea bass', 'pleasantly surprised', 'years ago', 'if you', 'new york', 'even though', 'gluten free', 'at least', 'iced tea', '15 minutes', 'come back', 'strip mall', 'make sure', 'our server', 'first time', 'com biz_photos', 'more than', 'www yelp', 'nothing special', 'dining room', 'yelp com', 'less than', 'last night', 'olive oil', 'go back', '20 minutes', '10 minutes', 'creme brulee', 'pulled pork', 'don know', 'spring rolls', 'drive thru', 'looking forward', 'pork belly', 'you re', 'per person', 'bloody mary', 'hands down', 'mac cheese', 'several times', 'dining experience', 'parking lot', 'wine list', 'hash browns', 'thin crust', 'too much', 'have been', 'cooked perfectly', 'customer service', 'will definitely', 'minutes later', 'saturday night', 'few times', 'eggs benedict', 'rib eye', 'every time', 'highly recommended', 'my favorite', 'late night', 'friday night', 'my husband', 'potato fries', 'main course', 'next door', 'deep fried', 'will be', 'you can', 'right away', 'coming back', '30 minutes', 'short ribs', 'much better', 'perfectly cooked']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigrams_split = [bigram.split(' ') for bigram in bigrams]\n",
    "\n",
    "smoothing_factors = [0.16275525810789912, 0.75, 0.85, 0.95, 0.99]\n",
    "for sf in smoothing_factors:\n",
    "    print(f\"Smoothing factor: {sf}\")\n",
    "    print(f\"Top 100 bigrams: {get_top100(sf)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2362e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rula bula', 'knick knacks', 'dac biet', 'ropa vieja', 'feng shui', 'cien agaves', 'gulab jamun', 'himal chuli', 'tammie coe', 'itty bitty', 'riff raff', 'khai hoan', 'roka akor', 'patatas bravas', 'nanay gloria', 'baskin robbins', 'puerto rican', 'reina pepiada', 'chicha morada', 'wal mart', 'dueling pianos', 'hoity toity', 'haricot vert', 'tutti santi', 'hodge podge', 'luc lac', 'lomo saltado', 'bradley ogden', 'nuoc mam', 'valle luna', 'hu tieu', 'alain ducasse', 'vice versa', 'porta alba', 'har gow', 'kao tod', 'pina colada', 'krispy kreme', 'artery clogging', 'pura vida', 'ore ida', 'chino bandido', 'sous vide', 'celine dion', 'holyrood 9a', 'lloyd wright', 'pin kaow', 'harry potter', 'molecular gastronomy', 'ping pang', 'casey moore', 'malai kofta', 'deja vu', 'cochinita pibil', 'aguas frescas', 'kilt lifter', 'lactose intolerant', 'hors oeuvres', 'moscow mule', 'ama ebi', 'yada yada', 'thit nuong', 'womp womp', 'yadda yadda', 'scantily clad', 'demi glace', 'duct tape', 'lindo michoacan', 'tres leches', 'kee mao', 'woody allen', 'arnold palmer', 'kool aid', 'cabo wabo', 'coca cola', 'osso bucco', 'dom demarco', 'bi bim', 'frou frou', 'salo salo', 'ritz carlton', 'bok choy', 'mt everest', 'stainless steel', 'rick moonen', 'khao soi', 'va bene', 'prik ong', 'nem nuong', 'turo turo', 'hush puppies', 'huli huli', 'jean philippe', 'tsk tsk', 'panna cotta', 'betty boop', 'van buren', 'bim bap', 'toby keith', 'hong kong']\n"
     ]
    }
   ],
   "source": [
    "top100 = get_top100(smoothing) # smoothing = 0.16275525810789912\n",
    "print(top100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
